{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "740a1079-012b-44aa-ae90-99b8cd916531"
      },
      "source": [
        "<p style=\"text-align:center\">\n",
        "    <a href=\"https://skills.network\" target=\"_blank\">\n",
        "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
        "    </a>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3718c28-7564-4598-8128-287f86e7a3bb"
      },
      "source": [
        "# **Lab: Implementing Q-Learning in Keras**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d0a51bb-aaa1-4baf-aadc-f9b92201616c"
      },
      "source": [
        "Estimated time needed: **30** minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f077be57-daea-4f2b-8400-cac0cee4ec8c"
      },
      "source": [
        "## Lab Overview\n",
        "In this lab, you will implement a Q-Learning algorithm using Keras to solve a reinforcement learning problem.\n",
        "\n",
        "## Learning objectives:\n",
        "By the end of this lab, you will:  \n",
        "- Implement a Q-Learning algorithm using Keras\n",
        "- Define and train a neural network to approximate the Q-values\n",
        "- Evaluate the performance of the trained Q-Learning agent\n",
        "\n",
        "## Prerequisites\n",
        "- Basic knowledge of Python programming\n",
        "- Familiarity with Keras and neural networks\n",
        "- Understanding of reinforcement learning concepts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e7ae585-d763-432e-9885-02e60f3c5828"
      },
      "source": [
        "### Step-by-Step Guide\n",
        "\n",
        "#### Step 1: Setting Up the Environment\n",
        "\n",
        "First, you will set up the environment using the OpenAI Gym library. You will use the 'CartPole-v1' environment, a common benchmark for reinforcement learning algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "62478a27-80de-4b61-adbc-4f74bc69c07e"
      },
      "outputs": [],
      "source": [
        "#%pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "49d88c32-7235-4c96-8a19-c699980d80fb"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade numpy==1.26.4\n",
        "#!pip uninstall tensorflow -y\n",
        "#!pip install tensorflow==2.16.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b13b1f86-0120-47f4-a600-d331bfcad203"
      },
      "source": [
        "### Set Environment Variables\n",
        "Sometimes, environment variables can help mitigate certain issues with TensorFlow. You can try disabling the oneDNN optimizations or CUDA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "497c0323-2b1b-4f5c-8b0a-abe682499a11"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5a53faa-3d1d-414a-96e9-d9246fa2d801"
      },
      "source": [
        "### Reduce Recursion Limit\n",
        "You can also try increasing the recursion limit, although this is generally more of a workaround than a solution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "19080fc8-e1c6-439d-8a81-7c8c79aad63a",
        "outputId": "40d7e967-dac7-4782-9c56-2226c67a3a3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[42]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import sys\n",
        "sys.setrecursionlimit(1500)\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "env.action_space.seed(42)\n",
        "env.observation_space.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "402129ab-20b1-4512-8457-e8ddd534c888"
      },
      "source": [
        "#### Explanation:  \n",
        "- `gym` is a toolkit for developing and comparing reinforcement learning algorithms.\n",
        "- `CartPole-v1` is an environment where a pole is balanced on a cart, and the goal is to prevent the pole from falling over.\n",
        "- Setting random seeds ensures that you can reproduce the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa8baa1b-4455-4fb8-805b-d8ded80a5a7b"
      },
      "source": [
        "### Step 2: Define the Q-Learning Model\n",
        "\n",
        "You will define a neural network using Keras to approximate the Q-values. The network will take the state as input and output Q-values for each action.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3216974c-9ee6-4150-9d23-aab484be2228"
      },
      "outputs": [],
      "source": [
        "# Suppress warnings for a cleaner notebook or console experience\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Override the default warning function\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "warnings.warn = warn\n",
        "\n",
        "# Import necessary libraries for the Q-Learning model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input  # Import Input layer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import gym  # Ensure the environment library is available\n",
        "\n",
        "# Define the model building function\n",
        "def build_model(state_size, action_size):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(state_size,)))  # Use Input layer to specify the input shape\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(action_size, activation='linear'))\n",
        "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
        "    return model\n",
        "\n",
        "# Create the environment and set up the model\n",
        "env = gym.make('CartPole-v1')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "model = build_model(state_size, action_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8a6d70f-0d38-4f7a-bfeb-9689553b943b"
      },
      "source": [
        "#### Explanation:\n",
        "- `Sequential` model: a linear stack of layers in Keras.\n",
        "- `Dense` layers: fully connected layers.\n",
        "- `input_dim`: the size of the input layer, corresponding to the state size.\n",
        "- `activation='relu'`: Rectified Linear Unit activation function.\n",
        "- `activation='linear'`: linear activation function for the output layer, as we are predicting continuous Q-values.\n",
        "- `Adam` optimizer: an optimization algorithm that adjusts the learning rate based on gradients.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eddf517-7ec8-4493-a362-a29bbfa1f48a"
      },
      "source": [
        "#### Step 3: Implement the Q-Learning Algorithm\n",
        "\n",
        "Now, you will implement the Q-Learning algorithm, which involves interacting with the environment, updating the Q-values, and training the neural network.\n",
        "\n",
        "**Define the replay Function**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "65799f7d-4168-406f-a551-5733986f960f",
        "outputId": "7b16f87f-4328-4edc-ba21-e3165df47fa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 1/10, score: 16, e: 1.0\n",
            "episode: 2/10, score: 18, e: 1.0\n",
            "episode: 3/10, score: 9, e: 1.0\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "episode: 4/10, score: 25, e: 0.99\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "episode: 5/10, score: 19, e: 0.95\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "episode: 6/10, score: 8, e: 0.93\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "episode: 7/10, score: 22, e: 0.89\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "episode: 8/10, score: 9, e: 0.87\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "episode: 9/10, score: 24, e: 0.83\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "episode: 10/10, score: 18, e: 0.79\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define epsilon and epsilon_decay\n",
        "epsilon = 1.0  # Starting with a high exploration rate\n",
        "epsilon_min = 0.01  # Minimum exploration rate\n",
        "epsilon_decay = 0.99  # Faster decay rate for epsilon after each episode\n",
        "\n",
        "# Replay memory\n",
        "memory = deque(maxlen=2000)\n",
        "\n",
        "def remember(state, action, reward, next_state, done):\n",
        "    \"\"\"Store experience in memory.\"\"\"\n",
        "    memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "def replay(batch_size=64):  # Increased batch size\n",
        "    \"\"\"Train the model using a random sample of experiences from memory.\"\"\"\n",
        "    if len(memory) < batch_size:\n",
        "        return  # Skip replay if there's not enough experience\n",
        "\n",
        "    minibatch = random.sample(memory, batch_size)  # Sample a random batch from memory\n",
        "\n",
        "    # Extract information for batch processing\n",
        "    states = np.vstack([x[0] for x in minibatch])\n",
        "    actions = np.array([x[1] for x in minibatch])\n",
        "    rewards = np.array([x[2] for x in minibatch])\n",
        "    next_states = np.vstack([x[3] for x in minibatch])\n",
        "    dones = np.array([x[4] for x in minibatch])\n",
        "\n",
        "    # Predict Q-values for the next states in batch\n",
        "    q_next = model.predict(next_states)\n",
        "    # Predict Q-values for the current states in batch\n",
        "    q_target = model.predict(states)\n",
        "\n",
        "    # Vectorized update of target values\n",
        "    for i in range(batch_size):\n",
        "        target = rewards[i]\n",
        "        if not dones[i]:\n",
        "            target += 0.95 * np.amax(q_next[i])  # Update Q value with the discounted future reward\n",
        "        q_target[i][actions[i]] = target  # Update only the taken action's Q value\n",
        "\n",
        "    # Train the model with the updated targets in batch\n",
        "    model.fit(states, q_target, epochs=1, verbose=0)  # Train in batch mode\n",
        "\n",
        "    # Reduce exploration rate (epsilon) after each training step\n",
        "    global epsilon\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "def act(state):\n",
        "    \"\"\"Choose an action based on the current state and exploration rate.\"\"\"\n",
        "    if np.random.rand() <= epsilon:\n",
        "        return random.randrange(action_size)  # Explore: choose a random action\n",
        "    act_values = model.predict(state)  # Exploit: predict action based on the state\n",
        "    return np.argmax(act_values[0])  # Return the action with the highest Q-value\n",
        "\n",
        "# Define the number of episodes you want to train the model for\n",
        "episodes = 10  # You can set this to any number you prefer\n",
        "train_frequency = 5  # Train the model every 5 steps\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()  # Unpack the tuple returned by env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    for time in range(200):  # Limit to 200 time steps per episode\n",
        "        action = act(state)\n",
        "        step_result = env.step(action)\n",
        "        if len(step_result) == 5:\n",
        "          next_state, reward, terminated, truncated, _ = step_result\n",
        "          done = terminated or truncated\n",
        "        else:\n",
        "          next_state, reward, done, _ = step_result\n",
        "        reward = reward if not done else -10\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        remember(state, action, reward, next_state, done)  # Store experience\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            print(f\"episode: {e+1}/{episodes}, score: {time}, e: {epsilon:.2}\")\n",
        "            break\n",
        "\n",
        "        # Train the model every 'train_frequency' steps\n",
        "        if time % train_frequency == 0:\n",
        "            replay(batch_size=64)  # Call replay with larger batch size for efficiency\n",
        "\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6ad0913-c421-4e99-8386-72ca3b1587a4"
      },
      "source": [
        "#### Step 4: Evaluate the Performance\n",
        "\n",
        "Finally, you will evaluate the performance of the trained Q-Learning agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "a79262c5-b77f-45c4-8579-ca2db386da0f",
        "outputId": "d1281aef-3414-4e04-f471-1f5e55cd5ecf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "episode: 1/10, score: 19\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "episode: 2/10, score: 29\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "episode: 3/10, score: 9\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "episode: 4/10, score: 58\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "episode: 5/10, score: 10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "episode: 6/10, score: 28\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "episode: 7/10, score: 32\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "episode: 8/10, score: 11\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "episode: 9/10, score: 21\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "episode: 10/10, score: 11\n"
          ]
        }
      ],
      "source": [
        "for e in range(10):\n",
        "\n",
        "    state = env.reset()  # Unpack the state from the tuple\n",
        "    state = np.reshape(state, [1, state_size])  # Reshape the state correctly\n",
        "    for time in range(500):\n",
        "        env.render()\n",
        "        action = np.argmax(model.predict(state)[0])\n",
        "        step_result = env.step(action)\n",
        "        if len(step_result) == 5:\n",
        "          next_state, reward, terminated, truncated, _ = step_result\n",
        "          done = terminated or truncated\n",
        "        else:\n",
        "          next_state, reward, done, _ = step_result\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        state = next_state\n",
        "        if done:\n",
        "            print(f\"episode: {e+1}/10, score: {time}\")\n",
        "            break\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f91d28c-8de5-4e38-81a6-c97c89fef59d"
      },
      "source": [
        "#### Explanation:\n",
        "- This loop runs 10 episodes to test the trained agent.\n",
        "- `env.render()`: visualizes the environment.\n",
        "- The agent chooses actions based on the trained model and interacts with the environment.\n",
        "- The score for each episode is printed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d250d8e-aba5-41ac-b595-87dff2c3bea6"
      },
      "source": [
        "# Practice exercises\n",
        "\n",
        "## Exercise 1: Experiment with Different Network Architectures\n",
        "\n",
        "### Objective:\n",
        "Understand how changing the architecture of the neural network affects the performance of the Q-Learning agent.\n",
        "\n",
        "### Instructions:\n",
        "1. Modify the `build_model()` function to include a different number of neurons and layers. For example, increase the number of layers to 3 and the number of neurons in each layer to 64.\n",
        "2. Train the model with the modified architecture and observe the performance in terms of average score achieved over 100 episodes.\n",
        "3. Compare the performance with the original architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "066076e0-cd7a-4a86-a1d0-1794238fc5ec",
        "outputId": "bb92cb8e-c4a8-4449-d97b-a2bdf5bf0ebc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with modified architecture (3 layers, 64 neurons each)...\n",
            "------------------------------------------------------------\n",
            "Episode: 10/100, Score: 17, Avg Score (last 10): 17.00, Epsilon: 0.878\n",
            "Episode: 20/100, Score: 18, Avg Score (last 10): 17.30, Epsilon: 0.722\n",
            "Episode: 30/100, Score: 35, Avg Score (last 10): 18.50, Epsilon: 0.591\n",
            "Episode: 40/100, Score: 43, Avg Score (last 10): 33.50, Epsilon: 0.414\n",
            "Episode: 50/100, Score: 198, Avg Score (last 10): 150.30, Epsilon: 0.090\n",
            "Episode: 60/100, Score: 247, Avg Score (last 10): 236.80, Epsilon: 0.010\n",
            "Episode: 70/100, Score: 192, Avg Score (last 10): 202.10, Epsilon: 0.010\n",
            "Episode: 80/100, Score: 238, Avg Score (last 10): 262.80, Epsilon: 0.010\n",
            "Episode: 90/100, Score: 226, Avg Score (last 10): 278.20, Epsilon: 0.010\n",
            "Episode: 100/100, Score: 298, Avg Score (last 10): 280.00, Epsilon: 0.010\n",
            "------------------------------------------------------------\n",
            "Training completed!\n",
            "Average score over all 100 episodes: 149.65\n",
            "Average score over last 10 episodes: 280.00\n",
            "Average score over last 50 episodes: 251.98\n",
            "Best score: 498\n",
            "Final epsilon: 0.010\n",
            "\n",
            "============================================================\n",
            "COMPARISON:\n",
            "============================================================\n",
            "Original Architecture: 2 hidden layers with 24 neurons each\n",
            "Modified Architecture: 3 hidden layers with 64 neurons each\n",
            "\n",
            "The modified architecture achieved an average score of 149.65 over 100 episodes.\n",
            "Generally, deeper and wider networks can learn more complex patterns,\n",
            "but may require more training time and can be more prone to overfitting.\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1: Experiment with Different Network Architectures\n",
        "\n",
        "# Initialize the environment\n",
        "env = gym.make('CartPole-v1')\n",
        "np.random.seed(42)\n",
        "env.action_space.seed(42)\n",
        "env.observation_space.seed(42)\n",
        "\n",
        "# Define state size and action size\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Modified build_model function with 3 layers and 64 neurons each\n",
        "def build_model(state_size, action_size):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(state_size,)))\n",
        "    model.add(Dense(64, activation='relu'))  # First hidden layer with 64 neurons\n",
        "    model.add(Dense(64, activation='relu'))  # Second hidden layer with 64 neurons\n",
        "    model.add(Dense(64, activation='relu'))  # Third hidden layer with 64 neurons\n",
        "    model.add(Dense(action_size, activation='linear'))\n",
        "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
        "    return model\n",
        "\n",
        "# Create the model with modified architecture\n",
        "model = build_model(state_size, action_size)\n",
        "\n",
        "# Epsilon-greedy parameters\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "# Replay memory\n",
        "memory = deque(maxlen=2000)\n",
        "\n",
        "def remember(state, action, reward, next_state, done):\n",
        "    \"\"\"Store experience in memory.\"\"\"\n",
        "    memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "def replay(batch_size=64):\n",
        "    \"\"\"Train the model using a random sample of experiences from memory.\"\"\"\n",
        "    if len(memory) < batch_size:\n",
        "        return\n",
        "\n",
        "    minibatch = random.sample(memory, batch_size)\n",
        "    states = np.vstack([x[0] for x in minibatch])\n",
        "    actions = np.array([x[1] for x in minibatch])\n",
        "    rewards = np.array([x[2] for x in minibatch])\n",
        "    next_states = np.vstack([x[3] for x in minibatch])\n",
        "    dones = np.array([x[4] for x in minibatch])\n",
        "\n",
        "    q_next = model.predict(next_states, verbose=0)\n",
        "    q_target = model.predict(states, verbose=0)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        target = rewards[i]\n",
        "        if not dones[i]:\n",
        "            target += 0.95 * np.amax(q_next[i])\n",
        "        q_target[i][actions[i]] = target\n",
        "\n",
        "    model.fit(states, q_target, epochs=1, verbose=0)\n",
        "\n",
        "    global epsilon\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "def act(state):\n",
        "    \"\"\"Choose an action based on the current state and exploration rate.\"\"\"\n",
        "    if np.random.rand() <= epsilon:\n",
        "        return random.randrange(action_size)\n",
        "    act_values = model.predict(state, verbose=0)\n",
        "    return np.argmax(act_values[0])\n",
        "\n",
        "# Training parameters\n",
        "episodes = 100  # Train for 100 episodes as specified\n",
        "train_frequency = 5\n",
        "scores = []  # Store scores for each episode\n",
        "\n",
        "print(\"Training with modified architecture (3 layers, 64 neurons each)...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Train the model with the modified architecture\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    total_reward = 0\n",
        "\n",
        "    for time in range(500):  # Maximum steps per episode\n",
        "        action = act(state)\n",
        "        step_result = env.step(action)\n",
        "        if len(step_result) == 5:\n",
        "          next_state, reward, terminated, truncated, _ = step_result\n",
        "          done = terminated or truncated\n",
        "        else:\n",
        "          next_state, reward, done, _ = step_result\n",
        "        reward = reward if not done else -10\n",
        "        total_reward += reward\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            scores.append(time)\n",
        "            if (e + 1) % 10 == 0:  # Print every 10 episodes\n",
        "                avg_score = np.mean(scores[-10:])\n",
        "                print(f\"Episode: {e+1}/{episodes}, Score: {time}, \"\n",
        "                      f\"Avg Score (last 10): {avg_score:.2f}, Epsilon: {epsilon:.3f}\")\n",
        "            break\n",
        "\n",
        "        if time % train_frequency == 0:\n",
        "            replay(batch_size=64)\n",
        "\n",
        "env.close()\n",
        "\n",
        "# Calculate and display final statistics\n",
        "print(\"-\" * 60)\n",
        "print(\"Training completed!\")\n",
        "print(f\"Average score over all {episodes} episodes: {np.mean(scores):.2f}\")\n",
        "print(f\"Average score over last 10 episodes: {np.mean(scores[-10:]):.2f}\")\n",
        "print(f\"Average score over last 50 episodes: {np.mean(scores[-50:]):.2f}\")\n",
        "print(f\"Best score: {max(scores)}\")\n",
        "print(f\"Final epsilon: {epsilon:.3f}\")\n",
        "\n",
        "# Comparison note\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COMPARISON:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Original Architecture: 2 hidden layers with 24 neurons each\")\n",
        "print(\"Modified Architecture: 3 hidden layers with 64 neurons each\")\n",
        "print(f\"\\nThe modified architecture achieved an average score of {np.mean(scores):.2f} over {episodes} episodes.\")\n",
        "print(\"Generally, deeper and wider networks can learn more complex patterns,\")\n",
        "print(\"but may require more training time and can be more prone to overfitting.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48437c03-37cc-4e43-b45f-62d1bd8402e8"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "# Install gym if necessary\n",
        "!pip install gym\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from keras.optimizers import Adam\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "# Initialize the environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Global settings\n",
        "episodes = 10  # Number of episodes\n",
        "batch_size = 32  # Size of the mini-batch for training\n",
        "memory = deque(maxlen=2000)  # Memory buffer to store experiences\n",
        "\n",
        "# Define state size and action size based on the environment\n",
        "state_size = env.observation_space.shape[0]  # State space size from the environment\n",
        "action_size = env.action_space.n  # Number of possible actions from the environment\n",
        "\n",
        "# Define the model\n",
        "def build_model(state_size, action_size):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(state_size,)))  # Explicit Input layer\n",
        "    model.add(Dense(32, activation='relu'))  # Smaller hidden layers\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(action_size, activation='linear'))\n",
        "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
        "    return model\n",
        "\n",
        "# Re-initialize the model with the new architecture\n",
        "model = build_model(state_size, action_size)\n",
        "\n",
        "# Placeholder for your action function (e.g., epsilon-greedy)\n",
        "def act(state):\n",
        "    return env.action_space.sample()  # For now, a random action is taken\n",
        "\n",
        "# Function to remember experiences in memory\n",
        "def remember(state, action, reward, next_state, done):\n",
        "    memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "# Optimized function to replay experiences from memory and train the model\n",
        "def replay(batch_size):\n",
        "    minibatch = random.sample(memory, batch_size)\n",
        "    states = np.vstack([sample[0] for sample in minibatch])\n",
        "    next_states = np.vstack([sample[3] for sample in minibatch])\n",
        "    targets = model.predict(states)\n",
        "    target_next = model.predict(next_states)\n",
        "    \n",
        "    for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
        "        target = reward if done else reward + 0.95 * np.amax(target_next[i])\n",
        "        targets[i][action] = target\n",
        "        \n",
        "    model.fit(states, targets, epochs=1, verbose=0)\n",
        "\n",
        "# Train the model with the modified architecture\n",
        "for e in range(episodes):\n",
        "    state, _ = env.reset()  # Unpack the state from the tuple\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    for time in range(200):  # Reduced number of steps per episode\n",
        "        action = act(state)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        reward = reward if not done else -10\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        \n",
        "        if done:\n",
        "            print(f\"episode: {e+1}/{episodes}, score: {time}\")\n",
        "            break\n",
        "        \n",
        "        if len(memory) > batch_size and time % 10 == 0:  # Train every 10 steps\n",
        "            replay(batch_size)  # Pass the batch size to replay()\n",
        "\n",
        "env.close()\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8328c23d-60e8-40fa-b4e8-13c7cafa624d"
      },
      "source": [
        "## Exercise 2: Implement an Adaptive Exploration Rate\n",
        "\n",
        "### Objective:\n",
        "Learn how to adapt the exploration rate (`epsilon`) based on the agent's performance to balance exploration and exploitation.\n",
        "\n",
        "### Instructions:\n",
        "1. Modify the `epsilon` decay strategy to decrease more rapidly when the agent's performance improves significantly.\n",
        "2. Implement a check to reduce `epsilon` faster if the agent achieves a score greater than a certain threshold (e.g., 200) in consecutive episodes.\n",
        "3. Observe the effect on the learning rate and the agent's performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "afe95cbf-5fc8-4780-9552-e12052b98289",
        "outputId": "703f8a2c-1864-4a28-dfa0-8fb944f3f393",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Adaptive Exploration Rate...\n",
            "============================================================\n",
            "Initial epsilon: 1.000\n",
            "Consecutive success threshold: 200\n",
            "============================================================\n",
            "Episode: 10/100, Score: 14, Avg Score (last 10): 20.60, Epsilon: 0.904, Consecutive Successes: 0\n",
            "Episode: 20/100, Score: 16, Avg Score (last 10): 24.00, Epsilon: 0.818, Consecutive Successes: 0\n",
            "Episode: 30/100, Score: 19, Avg Score (last 10): 21.10, Epsilon: 0.740, Consecutive Successes: 0\n",
            "Episode: 40/100, Score: 10, Avg Score (last 10): 14.10, Epsilon: 0.669, Consecutive Successes: 0\n",
            "Episode: 50/100, Score: 14, Avg Score (last 10): 14.50, Epsilon: 0.605, Consecutive Successes: 0\n",
            "Episode: 60/100, Score: 27, Avg Score (last 10): 13.60, Epsilon: 0.547, Consecutive Successes: 0\n",
            "Episode: 70/100, Score: 35, Avg Score (last 10): 36.80, Epsilon: 0.495, Consecutive Successes: 0\n",
            "Episode: 80/100, Score: 63, Avg Score (last 10): 45.30, Epsilon: 0.448, Consecutive Successes: 0\n",
            "Episode: 90/100, Score: 77, Avg Score (last 10): 61.60, Epsilon: 0.405, Consecutive Successes: 0\n",
            "Episode: 100/100, Score: 161, Avg Score (last 10): 142.00, Epsilon: 0.303, Consecutive Successes: 0\n",
            "\n",
            "============================================================\n",
            "Training completed!\n",
            "============================================================\n",
            "Average score over all 100 episodes: 39.36\n",
            "Average score over last 10 episodes: 142.00\n",
            "Average score over last 50 episodes: 59.86\n",
            "Best score: 301\n",
            "Final epsilon: 0.303\n",
            "Maximum consecutive successes: 0\n",
            "\n",
            "============================================================\n",
            "OBSERVATION:\n",
            "============================================================\n",
            "The adaptive epsilon decay strategy reduces exploration faster when\n",
            "the agent performs well, allowing it to exploit learned knowledge\n",
            "more quickly. This can lead to faster convergence and better\n",
            "final performance compared to fixed epsilon decay.\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2: Implement an Adaptive Exploration Rate\n",
        "\n",
        "# Reinitialize environment and model for this exercise\n",
        "env = gym.make('CartPole-v1')\n",
        "np.random.seed(42)\n",
        "env.action_space.seed(42)\n",
        "env.observation_space.seed(42)\n",
        "\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Build the model\n",
        "def build_model(state_size, action_size):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(state_size,)))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(action_size, activation='linear'))\n",
        "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
        "    return model\n",
        "\n",
        "model = build_model(state_size, action_size)\n",
        "\n",
        "# Initialize epsilon parameters\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.99\n",
        "\n",
        "# Replay memory\n",
        "memory = deque(maxlen=2000)\n",
        "\n",
        "def remember(state, action, reward, next_state, done):\n",
        "    \"\"\"Store experience in memory.\"\"\"\n",
        "    memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "def replay(batch_size=64):\n",
        "    \"\"\"Train the model using a random sample of experiences from memory.\"\"\"\n",
        "    if len(memory) < batch_size:\n",
        "        return\n",
        "\n",
        "    minibatch = random.sample(memory, batch_size)\n",
        "    states = np.vstack([x[0] for x in minibatch])\n",
        "    actions = np.array([x[1] for x in minibatch])\n",
        "    rewards = np.array([x[2] for x in minibatch])\n",
        "    next_states = np.vstack([x[3] for x in minibatch])\n",
        "    dones = np.array([x[4] for x in minibatch])\n",
        "\n",
        "    q_next = model.predict(next_states, verbose=0)\n",
        "    q_target = model.predict(states, verbose=0)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        target = rewards[i]\n",
        "        if not dones[i]:\n",
        "            target += 0.95 * np.amax(q_next[i])\n",
        "        q_target[i][actions[i]] = target\n",
        "\n",
        "    model.fit(states, q_target, epochs=1, verbose=0)\n",
        "\n",
        "def act(state):\n",
        "    \"\"\"Choose an action based on the current state and exploration rate.\"\"\"\n",
        "    if np.random.rand() <= epsilon:\n",
        "        return random.randrange(action_size)\n",
        "    act_values = model.predict(state, verbose=0)\n",
        "    return np.argmax(act_values[0])\n",
        "\n",
        "# Adaptive epsilon decay function\n",
        "def adjust_epsilon(score, consecutive_success_threshold=200):\n",
        "    \"\"\"\n",
        "    Adjust epsilon based on agent's performance.\n",
        "    Reduces epsilon faster when performance is good.\n",
        "    \"\"\"\n",
        "    global epsilon\n",
        "\n",
        "    if score >= consecutive_success_threshold:\n",
        "        # Reduce epsilon faster if performance is good (multiply by 0.9 instead of 0.99)\n",
        "        epsilon = max(epsilon_min, epsilon * 0.9)\n",
        "    else:\n",
        "        # Regular epsilon decay\n",
        "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "# Training parameters\n",
        "episodes = 100  # Train for 100 episodes to observe the effect\n",
        "train_frequency = 5\n",
        "batch_size = 64\n",
        "consecutive_success_threshold = 200  # Threshold for good performance\n",
        "consecutive_successes = 0  # Track consecutive episodes with score >= threshold\n",
        "\n",
        "scores = []  # Store scores for analysis\n",
        "\n",
        "print(\"Training with Adaptive Exploration Rate...\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Initial epsilon: {epsilon:.3f}\")\n",
        "print(f\"Consecutive success threshold: {consecutive_success_threshold}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Train the model with adaptive epsilon decay\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "\n",
        "    for time in range(500):  # Maximum steps per episode\n",
        "        action = act(state)\n",
        "        step_result = env.step(action)\n",
        "        if len(step_result) == 5:\n",
        "          next_state, reward, terminated, truncated, _ = step_result\n",
        "          done = terminated or truncated\n",
        "        else:\n",
        "          next_state, reward, done, _ = step_result\n",
        "        reward = reward if not done else -10\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            score = time\n",
        "            scores.append(score)\n",
        "\n",
        "            # Track consecutive successes\n",
        "            if score >= consecutive_success_threshold:\n",
        "                consecutive_successes += 1\n",
        "            else:\n",
        "                consecutive_successes = 0\n",
        "\n",
        "            # Adjust epsilon based on performance\n",
        "            adjust_epsilon(score, consecutive_success_threshold)\n",
        "\n",
        "            # Print progress every 10 episodes\n",
        "            if (e + 1) % 10 == 0:\n",
        "                avg_score = np.mean(scores[-10:])\n",
        "                print(f\"Episode: {e+1}/{episodes}, Score: {score}, \"\n",
        "                      f\"Avg Score (last 10): {avg_score:.2f}, \"\n",
        "                      f\"Epsilon: {epsilon:.3f}, \"\n",
        "                      f\"Consecutive Successes: {consecutive_successes}\")\n",
        "            break\n",
        "\n",
        "        # Train the model periodically\n",
        "        if time % train_frequency == 0:\n",
        "            replay(batch_size)\n",
        "\n",
        "env.close()\n",
        "\n",
        "# Display final statistics\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training completed!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Average score over all {episodes} episodes: {np.mean(scores):.2f}\")\n",
        "print(f\"Average score over last 10 episodes: {np.mean(scores[-10:]):.2f}\")\n",
        "print(f\"Average score over last 50 episodes: {np.mean(scores[-50:]):.2f}\")\n",
        "print(f\"Best score: {max(scores)}\")\n",
        "print(f\"Final epsilon: {epsilon:.3f}\")\n",
        "print(f\"Maximum consecutive successes: {consecutive_successes}\")\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"OBSERVATION:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"The adaptive epsilon decay strategy reduces exploration faster when\")\n",
        "print(\"the agent performs well, allowing it to exploit learned knowledge\")\n",
        "print(\"more quickly. This can lead to faster convergence and better\")\n",
        "print(\"final performance compared to fixed epsilon decay.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b13daec-115f-4805-a42e-545e935fefb2"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "# Function to adjust epsilon based on performance\n",
        "def adjust_epsilon(score, consecutive_success_threshold=200):\n",
        "    global epsilon\n",
        "\n",
        "    if score >= consecutive_success_threshold:\n",
        "        epsilon = max(epsilon_min, epsilon * 0.9)  # Reduce epsilon faster if performance is good\n",
        "    else:\n",
        "        epsilon = max(epsilon_min, epsilon * epsilon_decay)  # Regular epsilon decay\n",
        "\n",
        "episodes = 2  # Set number of episodes for training\n",
        "\n",
        "# Train the model with adaptive epsilon decay\n",
        "for e in range(episodes):\n",
        "    state = env.reset()  \n",
        "    state = state[0]  # Extract the first element, which is the actual state array\n",
        "    state = np.reshape(state, [1, len(state)])  # Reshape state to match the expected input shape\n",
        "\n",
        "    total_reward = 0\n",
        "\n",
        "    for time in range(500):  # Limit the episode to 500 time steps\n",
        "        action = act(state)  # Choose action based on policy\n",
        "        next_state, reward, done, truncated, _ = env.step(action)  # Unpack 5 values\n",
        "\n",
        "        reward = reward if not done else -10  # Penalize for reaching a terminal state\n",
        "        total_reward += reward  # Accumulate rewards\n",
        "\n",
        "        next_state = np.reshape(next_state, [1, len(next_state)])  # Reshape next state (optional based on model needs)\n",
        "\n",
        "        remember(state, action, reward, next_state, done)  # Store experience in memory\n",
        "        state = next_state  # Update the current state\n",
        "\n",
        "        if done or truncated:  # Check if the episode is done or truncated\n",
        "            adjust_epsilon(total_reward)  # Adjust epsilon based on the total reward\n",
        "            print(f\"episode: {e}/{episodes}, score: {time}, e: {epsilon:.2}\")  # Print the episode details\n",
        "            break  # Break out of the loop if the episode is done or truncated\n",
        "\n",
        "        if len(memory) > batch_size:  # Check if enough experiences are stored in memory\n",
        "            replay(batch_size)  # Train the model with the stored experiences (pass batch_size here)\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa344ad6-9b6d-492c-a9f8-e7f7e01e0657"
      },
      "source": [
        "## Exercise 3 : Implement a Custom Reward Function\n",
        "\n",
        "### Objective:\n",
        "Understand the impact of reward shaping on training the Q-Learning agent.\n",
        "\n",
        "### Instructions:\n",
        "1. Modify the reward function to provide more granular feedback to the agent. For example, give higher rewards for keeping the pole more vertical and closer to the center.\n",
        "2. Implement a reward function that rewards the agent proportionally to the angle of the pole and the distance of the cart from the center.\n",
        "3. Train the agent with the new reward function and compare the learning speed and stability to the original setting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "87702a6c-6494-4286-9d88-cc2e66294644",
        "outputId": "7cf68197-6b53-4d85-ed79-67a1f3ce99db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Custom Reward Function...\n",
            "============================================================\n",
            "Custom reward: 0.5 * (1 - |x|/2.4) + 0.5 * (1 - |theta|/0.2095)\n",
            "============================================================\n",
            "Episode: 10/100, Score: 11, Avg Score (last 10): 19.90, Total Reward: -1.43, Avg Reward (last 10): 6.71, Epsilon: 0.740\n",
            "Episode: 20/100, Score: 9, Avg Score (last 10): 13.90, Total Reward: -2.53, Avg Reward (last 10): 1.42, Epsilon: 0.536\n",
            "Episode: 30/100, Score: 9, Avg Score (last 10): 11.30, Total Reward: -2.74, Avg Reward (last 10): -0.92, Epsilon: 0.409\n",
            "Episode: 40/100, Score: 60, Avg Score (last 10): 22.00, Total Reward: 40.80, Avg Reward (last 10): 8.18, Epsilon: 0.257\n",
            "Episode: 50/100, Score: 29, Avg Score (last 10): 30.60, Total Reward: 13.37, Avg Reward (last 10): 13.99, Epsilon: 0.134\n",
            "Episode: 60/100, Score: 50, Avg Score (last 10): 34.90, Total Reward: 29.64, Avg Reward (last 10): 18.36, Epsilon: 0.064\n",
            "Episode: 70/100, Score: 58, Avg Score (last 10): 67.50, Total Reward: 33.62, Avg Reward (last 10): 45.58, Epsilon: 0.016\n",
            "Episode: 80/100, Score: 304, Avg Score (last 10): 159.90, Total Reward: 221.93, Avg Reward (last 10): 124.65, Epsilon: 0.010\n",
            "Episode: 90/100, Score: 190, Avg Score (last 10): 393.00, Total Reward: 154.08, Avg Reward (last 10): 325.19, Epsilon: 0.010\n",
            "Episode: 100/100, Score: 499, Avg Score (last 10): 472.30, Total Reward: 434.02, Avg Reward (last 10): 393.90, Epsilon: 0.010\n",
            "\n",
            "============================================================\n",
            "Training completed with Custom Reward Function!\n",
            "============================================================\n",
            "Average score over all 100 episodes: 122.53\n",
            "Average score over last 10 episodes: 472.30\n",
            "Average score over last 50 episodes: 225.52\n",
            "Best score: 499\n",
            "Final epsilon: 0.010\n",
            "\n",
            "Average total reward per episode: 93.71\n",
            "Average total reward (last 10 episodes): 393.90\n",
            "\n",
            "============================================================\n",
            "COMPARISON: Custom Reward vs Original Reward\n",
            "============================================================\n",
            "Original Reward Function:\n",
            "  - Reward = 1.0 for each step (until episode ends)\n",
            "  - Reward = -10 when episode terminates\n",
            "\n",
            "Custom Reward Function:\n",
            "  - Reward = 0.5 * (1 - |x|/2.4) + 0.5 * (1 - |theta|/0.2095)\n",
            "  - Provides granular feedback based on cart position and pole angle\n",
            "  - Reward = -10 when episode terminates\n",
            "\n",
            "Expected Benefits:\n",
            "  - Faster learning: Agent receives immediate feedback on its actions\n",
            "  - More stable training: Continuous rewards help guide learning\n",
            "  - Better convergence: Agent learns to optimize for both objectives\n",
            "\n",
            "The custom reward function achieved an average score of 122.53\n",
            "over 100 episodes, with the last 10 episodes averaging 472.30.\n"
          ]
        }
      ],
      "source": [
        "# Exercise 3: Implement a Custom Reward Function\n",
        "\n",
        "# Reinitialize environment and model for this exercise\n",
        "env = gym.make('CartPole-v1')\n",
        "np.random.seed(42)\n",
        "env.action_space.seed(42)\n",
        "env.observation_space.seed(42)\n",
        "\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Build the model\n",
        "def build_model(state_size, action_size):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(state_size,)))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(action_size, activation='linear'))\n",
        "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
        "    return model\n",
        "\n",
        "model = build_model(state_size, action_size)\n",
        "\n",
        "# Initialize epsilon parameters\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.99\n",
        "\n",
        "# Replay memory\n",
        "memory = deque(maxlen=2000)\n",
        "\n",
        "def remember(state, action, reward, next_state, done):\n",
        "    \"\"\"Store experience in memory.\"\"\"\n",
        "    memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "def replay(batch_size=64):\n",
        "    \"\"\"Train the model using a random sample of experiences from memory.\"\"\"\n",
        "    if len(memory) < batch_size:\n",
        "        return\n",
        "\n",
        "    minibatch = random.sample(memory, batch_size)\n",
        "    states = np.vstack([x[0] for x in minibatch])\n",
        "    actions = np.array([x[1] for x in minibatch])\n",
        "    rewards = np.array([x[2] for x in minibatch])\n",
        "    next_states = np.vstack([x[3] for x in minibatch])\n",
        "    dones = np.array([x[4] for x in minibatch])\n",
        "\n",
        "    q_next = model.predict(next_states, verbose=0)\n",
        "    q_target = model.predict(states, verbose=0)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        target = rewards[i]\n",
        "        if not dones[i]:\n",
        "            target += 0.95 * np.amax(q_next[i])\n",
        "        q_target[i][actions[i]] = target\n",
        "\n",
        "    model.fit(states, q_target, epochs=1, verbose=0)\n",
        "\n",
        "    global epsilon\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "def act(state):\n",
        "    \"\"\"Choose an action based on the current state and exploration rate.\"\"\"\n",
        "    if np.random.rand() <= epsilon:\n",
        "        return random.randrange(action_size)\n",
        "    act_values = model.predict(state, verbose=0)\n",
        "    return np.argmax(act_values[0])\n",
        "\n",
        "# Custom reward function\n",
        "def custom_reward(state):\n",
        "    \"\"\"\n",
        "    Custom reward function that provides granular feedback based on:\n",
        "    - Cart position (x): closer to center = higher reward\n",
        "    - Pole angle (theta): more vertical = higher reward\n",
        "\n",
        "    State variables:\n",
        "    - x: cart position (range approximately -2.4 to 2.4)\n",
        "    - x_dot: cart velocity\n",
        "    - theta: pole angle in radians (range approximately -0.2095 to 0.2095, about ±12 degrees)\n",
        "    - theta_dot: pole angular velocity\n",
        "    \"\"\"\n",
        "    # Extract state variables\n",
        "    # Note: state is already a 1D array, so we need to flatten if it's reshaped\n",
        "    if len(state.shape) > 1:\n",
        "        state_flat = state[0]  # Extract from reshaped state\n",
        "    else:\n",
        "        state_flat = state\n",
        "\n",
        "    x, x_dot, theta, theta_dot = state_flat\n",
        "\n",
        "    # Normalize and compute rewards\n",
        "    # Cart position reward: maximum when x=0, decreases as |x| increases\n",
        "    # Using 2.4 as the approximate limit (CartPole-v1 limit is 2.4)\n",
        "    cart_reward = 1.0 - abs(x) / 2.4\n",
        "\n",
        "    # Pole angle reward: maximum when theta=0 (vertical), decreases as |theta| increases\n",
        "    # Using 0.2095 radians (approximately 12 degrees) as the threshold\n",
        "    # This is close to the failure threshold in CartPole-v1\n",
        "    angle_reward = 1.0 - abs(theta) / 0.2095\n",
        "\n",
        "    # Combine rewards (weighted sum)\n",
        "    # You can adjust weights to emphasize one aspect over the other\n",
        "    total_reward = 0.5 * cart_reward + 0.5 * angle_reward\n",
        "\n",
        "    # Scale to make it comparable to original reward (which is 1.0 per step)\n",
        "    # This ensures the reward magnitude is similar\n",
        "    return total_reward\n",
        "\n",
        "# Training parameters\n",
        "episodes = 100  # Train for 100 episodes to observe learning speed and stability\n",
        "train_frequency = 5\n",
        "batch_size = 64\n",
        "\n",
        "scores = []  # Store scores for analysis\n",
        "rewards_per_episode = []  # Store total rewards per episode\n",
        "\n",
        "print(\"Training with Custom Reward Function...\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Custom reward: 0.5 * (1 - |x|/2.4) + 0.5 * (1 - |theta|/0.2095)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Train the model with custom reward function\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    total_reward = 0\n",
        "\n",
        "    for time in range(500):  # Maximum steps per episode\n",
        "        action = act(state)\n",
        "        step_result = env.step(action)\n",
        "        if len(step_result) == 5:\n",
        "          next_state, _, terminated, truncated, _ = step_result\n",
        "          done = terminated or truncated\n",
        "        else:\n",
        "          next_state, _, done, _ = step_result\n",
        "\n",
        "\n",
        "        # Use custom reward function\n",
        "        reward = custom_reward(next_state) if not done else -10\n",
        "        total_reward += reward\n",
        "\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            score = time\n",
        "            scores.append(score)\n",
        "            rewards_per_episode.append(total_reward)\n",
        "\n",
        "            # Print progress every 10 episodes\n",
        "            if (e + 1) % 10 == 0:\n",
        "                avg_score = np.mean(scores[-10:])\n",
        "                avg_reward = np.mean(rewards_per_episode[-10:])\n",
        "                print(f\"Episode: {e+1}/{episodes}, Score: {score}, \"\n",
        "                      f\"Avg Score (last 10): {avg_score:.2f}, \"\n",
        "                      f\"Total Reward: {total_reward:.2f}, \"\n",
        "                      f\"Avg Reward (last 10): {avg_reward:.2f}, \"\n",
        "                      f\"Epsilon: {epsilon:.3f}\")\n",
        "            break\n",
        "\n",
        "        # Train the model periodically\n",
        "        if time % train_frequency == 0:\n",
        "            replay(batch_size)\n",
        "\n",
        "env.close()\n",
        "\n",
        "# Display final statistics\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training completed with Custom Reward Function!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Average score over all {episodes} episodes: {np.mean(scores):.2f}\")\n",
        "print(f\"Average score over last 10 episodes: {np.mean(scores[-10:]):.2f}\")\n",
        "print(f\"Average score over last 50 episodes: {np.mean(scores[-50:]):.2f}\")\n",
        "print(f\"Best score: {max(scores)}\")\n",
        "print(f\"Final epsilon: {epsilon:.3f}\")\n",
        "print(f\"\\nAverage total reward per episode: {np.mean(rewards_per_episode):.2f}\")\n",
        "print(f\"Average total reward (last 10 episodes): {np.mean(rewards_per_episode[-10:]):.2f}\")\n",
        "\n",
        "# Comparison with original reward function\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COMPARISON: Custom Reward vs Original Reward\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Original Reward Function:\")\n",
        "print(\"  - Reward = 1.0 for each step (until episode ends)\")\n",
        "print(\"  - Reward = -10 when episode terminates\")\n",
        "print(\"\\nCustom Reward Function:\")\n",
        "print(\"  - Reward = 0.5 * (1 - |x|/2.4) + 0.5 * (1 - |theta|/0.2095)\")\n",
        "print(\"  - Provides granular feedback based on cart position and pole angle\")\n",
        "print(\"  - Reward = -10 when episode terminates\")\n",
        "print(\"\\nExpected Benefits:\")\n",
        "print(\"  - Faster learning: Agent receives immediate feedback on its actions\")\n",
        "print(\"  - More stable training: Continuous rewards help guide learning\")\n",
        "print(\"  - Better convergence: Agent learns to optimize for both objectives\")\n",
        "print(f\"\\nThe custom reward function achieved an average score of {np.mean(scores):.2f}\")\n",
        "print(f\"over {episodes} episodes, with the last 10 episodes averaging {np.mean(scores[-10:]):.2f}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9beae2d-b11d-4774-892a-f1c91be1ef27"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "# Define a custom reward function based on the cart position and pole angle\n",
        "def custom_reward(state):\n",
        "    # Extract state variables: x (cart position), x_dot (cart velocity), theta (pole angle), theta_dot (pole angular velocity)\n",
        "    x, x_dot, theta, theta_dot = state\n",
        "    \n",
        "    # Custom reward function: Encourage the agent to keep the cart near the center and the pole upright\n",
        "    reward = (1 - abs(x) / 2.4) + (1 - abs(theta) / 0.20948)\n",
        "    \n",
        "    return reward\n",
        "\n",
        "episodes = 2  # Number of episodes to run\n",
        "\n",
        "# Train the model with the custom reward function\n",
        "for e in range(episodes):\n",
        "    state = env.reset()  # Reset the environment\n",
        "\n",
        "    # Print the state structure for debugging\n",
        "    print(f\"State: {state}, State Type: {type(state)}\")\n",
        "\n",
        "    # Extract the state if it's a tuple and reshape if necessary\n",
        "    if isinstance(state, tuple):\n",
        "        state = state[0]  # Extract the first element if it's a tuple\n",
        "\n",
        "    state = np.reshape(state, [1, state_size])  # Reshape state to match the expected input shape\n",
        "\n",
        "    for time in range(500):  # Limit the episode to 500 time steps\n",
        "        action = act(state)  # Choose an action based on the current state\n",
        "        \n",
        "        # Unpack 5 values returned by env.step(action)\n",
        "        next_state, reward, done, truncated, _ = env.step(action)\n",
        "\n",
        "        # Compute the custom reward based on the next state\n",
        "        reward = custom_reward(next_state) if not done else -10\n",
        "\n",
        "        # Reshape next_state if necessary\n",
        "        if isinstance(next_state, tuple):\n",
        "            next_state = next_state[0]  # Extract the first element if it's a tuple\n",
        "\n",
        "        next_state = np.reshape(next_state, [1, state_size])  # Reshape next state to match input shape\n",
        "\n",
        "        # Store the experience in memory\n",
        "        remember(state, action, reward, next_state, done)\n",
        "        state = next_state  # Update the current state\n",
        "\n",
        "        if done or truncated:  # If the episode is done, break out of the loop\n",
        "            print(f\"episode: {e}/{episodes}, score: {time}, e: {epsilon:.2}\")\n",
        "            break\n",
        "\n",
        "        if len(memory) > batch_size:  # If there are enough samples in memory, train the model\n",
        "            replay(batch_size)  # Train the model with a batch of experiences\n",
        "\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc8bb10d-660d-4974-b338-308c2f929d76"
      },
      "source": [
        "#### Conclusion\n",
        "\n",
        "Congratulations on completing this lab!  In this lab, you explored various strategies to enhance the performance of the Q-Learning agent, such as experimenting with different network architectures, implementing adaptive exploration rates, and customizing the reward function. These variations help reinforce your understanding of the Q-Learning algorithm's flexibility and the impact of different hyperparameters and strategies on the learning process.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5f9c898-718c-4391-8889-a252e1f327eb"
      },
      "source": [
        "## Authors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b79de45-a9fe-4091-9c76-06da4a1115a2"
      },
      "source": [
        "Skills Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68a7ee5c-a336-47ce-86ca-f6919fc5cf58"
      },
      "source": [
        "Copyright © IBM Corporation. All rights reserved.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "prev_pub_hash": "c7aa20b200c76f649b9de38f08b4d27f83eff96f5933b3756055c6d8d0fb5867",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}